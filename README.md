# Text Preprocessing Pipeline

## Overview

The **Text Preprocessing Pipeline** is a modular and reusable Python toolkit designed to clean and prepare raw text data for Natural Language Processing (NLP) tasks. By handling common preprocessing steps such as tokenization, lowercasing, stopword removal, stemming, and special character removal, this pipeline ensures that your text data is in an optimal format for further analysis or model training.

## Requirments
- **Python 3.8 or higher**
- **NLTK**

## Features

- **Tokenization**: Splits text into individual words.
- **Lowercasing**: Converts all text to lowercase for normalization.
- **Stopword Removal**: Eliminates common words that do not add significant meaning.
- **Stemming**: Reduces words to their root form using Snowball Stemmer.
- **Lemmatization**: (Optional) Converts words to their dictionary form.
- **Special Character Removal**: Strips out unwanted characters and punctuation.
- **Modular Design**: Easily extendable and reusable across different NLP projects.

## Problem Statement

In Natural Language Processing (NLP), raw text data often contains noise such as special characters, stopwords, inconsistent capitalizations, and grammatical variations of words. These elements can negatively impact the performance of machine learning models. Therefore, it's essential to preprocess text data to remove irrelevant information and standardize the format, ensuring that models are trained on clean and meaningful data.

**Objective**: Develop a reusable and modular **Text Preprocessing Pipeline** that effectively cleans and prepares raw text data for various NLP tasks, enhancing the performance and accuracy of downstream models.

## Installation

1. **Clone the Repository**
2. **Run nltk_resources.py**
3. **run TextPreprocessing.py**